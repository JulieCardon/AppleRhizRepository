---
title: "DADA_2 Apples"
author: "Julie Cardon"
date: "2024-03-13"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.align = "center",
<<<<<<< HEAD
                      fig.path = "./figures/01_DADA_2/") # send any figure output to this folder 
=======
                      fig.path = "../figures/01_DADA_2/") # send any figure output to this folder 
>>>>>>> 0debb33ca2b2f00a0a0c3f1f72ec87fd1329555c
```

#Apple Rhizosphere 16S File Processing with DADA2

## Load Libraries
```{r load-libraries}
#install.packages("devtools")
library("devtools")
#devtools::install_github("thomasp85/patchwork@HEAD")
library(patchwork)
#install.packages("pacman")
library(pacman)
pacman::p_load(tidyverse, BiocManager, devtools, dada2, 
               phyloseq, patchwork, DT, iNEXT, vegan,
               install = TRUE)
```


# Set Seed
```{r}
set.seed(092819)
```

# For 2022 Data

# Load Data
```{r load-data-2022}
# Set the raw fastq path to the raw sequencing files 
# Path to the fastq files 
raw_fastqs_path2 <- "01_DADA_2/data/rawfastqs/data_2022"
raw_fastqs_path2
# What files are in this path? Intuition Check 
head(list.files(raw_fastqs_path2))
# How many files are there? 
str(list.files(raw_fastqs_path2))
```

# Create vector of forward reads
```{r forward-reads-2022}
forward_reads2 <- list.files(raw_fastqs_path2, pattern = "R1.fastq.gz", full.names = TRUE)  
# Intuition Check 
head(forward_reads2)  
```

# Create a vector of reverse reads
```{r reverse-reads-2022}
reverse_reads2 <- list.files(raw_fastqs_path2, pattern = "R2.fastq.gz", full.names = TRUE)
head(reverse_reads2)
```

# Assess Raw Read Quality

## Evaluate raw sequence quality prior to trimming
### Plot 12 random samples of plots 
```{r raw-quality-plot-2022, fig.width=12, fig.height=8}
# Randomly select 12 samples from dataset to evaluate 
random_samples2 <- sample(1:length(reverse_reads2), size = 12)
random_samples2

# Calculate and plot quality of these twelve samples
forward_plot_12_2 <- plotQualityProfile(forward_reads2[random_samples2]) + 
  labs(title = "Forward Read: Raw Quality")

reverse_plot_12_2 <- plotQualityProfile(reverse_reads2[random_samples2]) + 
  labs(title = "Reverse Read: Raw Quality 2022")


# Plot them together with patchwork
forward_plot_12_2 + reverse_plot_12_2
```

### Aggregated Raw Quality Plots 
```{r raw-aggregate-plot-2022, fig.width=5.5, fig.height=3.5}
# Aggregate all QC plots 
# Forward reads
forward_preQC_plot2 <- 
  plotQualityProfile(forward_reads2, aggregate = TRUE) + 
  labs(title = "Forward Pre-QC 2022")

# reverse reads
reverse_preQC_plot2 <- 
  plotQualityProfile(reverse_reads2, aggregate = TRUE) + 
  labs(title = "Reverse Pre-QC 2022")

preQC_aggregate_plot2 <- 
  # Plot the forward and reverse together 
  forward_preQC_plot2 + reverse_preQC_plot2

# Show the plot
preQC_aggregate_plot2
```

The quality of the 2022 reads leaves a bit to be desired.  We don't want to truncate too much, but a bit at the beginning, and then the primer on the other end can be truncated.  Filtering should help the quality a bit.


## Prepare a placeholder for filtered reads 
```{r prep-filtered-sequences-2022}
# vector of our samples, extract sample name from files 
samples2 <- paste0(sapply(strsplit(basename(forward_reads2), "_"), `[`,7), "_2022") 
# Intuition Check 
head(samples2)

# Place filtered reads into filtered_fastqs_path
filtered_fastqs_path2 <- "01_DADA_2/data/02_filtered_fastqs"
filtered_fastqs_path2

# create 2 variables: filtered_F, filtered_R
filtered_forward_reads2 <- 
  file.path(filtered_fastqs_path2, paste0(samples2, "_R1_filtered_2022.fastq.gz"))
length(filtered_forward_reads2)

# reverse reads
filtered_reverse_reads2 <- 
  file.path(filtered_fastqs_path2, paste0(samples2, "_R2_filtered_2022.fastq.gz"))
length(filtered_reverse_reads2)
```

<<<<<<< HEAD
=======
```{r}
table(filtered_forward_reads2)
```



>>>>>>> 0debb33ca2b2f00a0a0c3f1f72ec87fd1329555c
# Filter and Trim Reads
```{r Filter-trim-2022}
filtered_reads2<-filterAndTrim(forward_reads2, filtered_forward_reads2,
              reverse_reads2, filtered_reverse_reads2,   
              truncLen = c(247,247), trimLeft = c(17,22),
              maxN = 0, maxEE = c(2,2), truncQ = 2, 
              rm.phix = TRUE, compress = TRUE, 
              multithread = TRUE)
```

# Assess Trimmed Read Quality
```{r filterTrim-quality-plots-2022,  fig.width=12, fig.height=8}
# Plot the 12 random samples after QC
forward_filteredQual_plot_12_2 <- 
  plotQualityProfile(filtered_forward_reads2[random_samples2]) + 
  labs(title = "Trimmed Forward Read Quality 2022")

reverse_filteredQual_plot_12_2 <- 
  plotQualityProfile(filtered_reverse_reads2[random_samples2]) + 
  labs(title = "Trimmed Reverse Read Quality 2022")

# Put the two plots together 
forward_filteredQual_plot_12_2 + reverse_filteredQual_plot_12_2
```

## Aggregated Trimmed Plots 
```{r qc-aggregate-plot-2022, fig.width=5.5, fig.height=3.5}
# Aggregate all QC plots 
# Forward reads
forward_postQC_plot2 <- 
  plotQualityProfile(filtered_forward_reads2, aggregate = TRUE) + 
  labs(title = "Forward Post-QC 2022")

# reverse reads
reverse_postQC_plot2 <- 
  plotQualityProfile(filtered_reverse_reads2, aggregate = TRUE) + 
  labs(title = "Reverse Post-QC 2022")

postQC_aggregate_plot2 <- 
  # Plot the forward and reverse together 
  forward_postQC_plot2 + reverse_postQC_plot2

# Show the plot
postQC_aggregate_plot2
```


Here, we see the sequences are improved from before. The very low quality that we saw in the pre-QC plots is mostly gone. There is some low quality towards the end, but it will hopefully overlap with the good quality sections of the opposite read. 

<<<<<<< HEAD
=======

>>>>>>> 0debb33ca2b2f00a0a0c3f1f72ec87fd1329555c
## Stats on read output from `filterAndTrim`

```{r filterTrim-stats-2022}
# Make output into dataframe 
filtered_df2 <- as.data.frame(filtered_reads2)
head(filtered_df2)


# calculate some stats 
filtered_df2 %>%
  reframe(median_reads_in2 = median(reads.in),
          median_reads_out2 = median(reads.out),
          median_percent_retained2 = (median(reads.out)/median(reads.in)))
```

About 60% of the reads were retained, this seems sufficient since sequencing depth was quite high.

### Visualize QC differences in plot 
```{r pre-post-QC-plot-2022, fig.width=6, fig.height=5.5}
# Plot the pre and post together in one plot
preQC_aggregate_plot2 / postQC_aggregate_plot2
```

# Error Modelling 

## Learn the errors 
```{r learn-errors-2022, fig.width=12, fig.height=8}
# Forward reads 
error_forward_reads2 <- 
  learnErrors(filtered_forward_reads2, multithread = 5)
# Plot Forward  
forward_error_plot2 <- 
  plotErrors(error_forward_reads2, nominalQ = 5) + 
  labs(title = "Forward Read Error Model 2022")

# Reverse reads 
error_reverse_reads2 <- 
  learnErrors(filtered_reverse_reads2, multithread = 5)
# Plot reverse
reverse_error_plot2 <- 
  plotErrors(error_reverse_reads2, nominalQ = 5) + 
  labs(title = "Reverse Read Error Model 2022")

# Put the two plots together
forward_error_plot2 + reverse_error_plot2
```

The black lines and dots seem to overlap.  And the red lines are negative

<<<<<<< HEAD
=======

>>>>>>> 0debb33ca2b2f00a0a0c3f1f72ec87fd1329555c
# Infer ASVs 

**An important note:** This process occurs separately on forward and reverse reads! This is quite a different approach from how OTUs are identified in Mothur and also from UCHIME, oligotyping, and other OTU, MED, and ASV approaches.

```{r infer-ASVs-2022}
# Infer ASVs on the forward sequences
dada_forward2 <- dada(filtered_forward_reads2,
                     err = error_forward_reads2, 
                     multithread = TRUE)

typeof(dada_forward2)
# Grab a sample and look at it 
dada_forward2$`20211005-MA-CWS1P_R1_filtered.fastq.gz`


# Infer ASVs on the reverse sequences 
dada_reverse2 <- dada(filtered_reverse_reads2,
                     err = error_reverse_reads2,
                     multithread = TRUE)

# Inspect 
dada_reverse2[1]
dada_reverse2[30]
```

# Merge Forward & Reverse ASVs

Now, merge the forward and reverse ASVs into contigs. 

```{r merge-ASVs}
# merge forward and reverse ASVs
merged_ASVs2 <- mergePairs(dada_forward2, filtered_forward_reads2, 
                          dada_reverse2, filtered_reverse_reads2,
                          verbose = TRUE)

# Evaluate the output 
typeof(merged_ASVs2)
length(merged_ASVs2)
names(merged_ASVs2)

# Inspect the merger data.frame from the 20210602-MA-ABB1P 
head(merged_ASVs2[[3]])
str(merged_ASVs2)
```

# Create Raw ASV Count Table 
```{r generate-ASV-table, fig.width=3.5, fig.height=3}
# Create the ASV Count Table 
raw_ASV_table2 <- makeSequenceTable(merged_ASVs2)

# Write out the file to data/01_DADA2

#filtered_fastqs_path2 <- "01_DADA_2/data/02_filtered_fastqs"
#raw_ASV_table2_path <-

# Check the type and dimensions of the data
dim(raw_ASV_table2)
class(raw_ASV_table2)
typeof(raw_ASV_table2)

# Inspect the distribution of sequence lengths of all ASVs in dataset 
table(nchar(getSequences(raw_ASV_table2)))

# Inspect the distribution of sequence lengths of all ASVs in dataset 
# AFTER TRIM
data.frame(Seq_Length = nchar(getSequences(raw_ASV_table2))) %>%
  ggplot(aes(x = Seq_Length )) + 
  geom_histogram() + 
  labs(title = "Raw distribution of ASV length")
```

###################################################
###################################################
# TRIM THE ASVS
<<<<<<< HEAD
# Let's trim the ASVs to only be the right size, which is somewhere in the neighborhood of 400 (its from the v3 v4 region, which can vary in length depending on the taxon)
=======
# Let's trim the ASVs to only be the right size, which is 249.
# 249 originates from our expected amplicon of 252 - 3bp in the forward read due to low quality.
>>>>>>> 0debb33ca2b2f00a0a0c3f1f72ec87fd1329555c
``` {r trim-ASVs-2022}
# We will allow for a few 
raw_ASV_table_trimmed2 <- raw_ASV_table2[,nchar(colnames(raw_ASV_table2)) %in% 399:436]

# Inspect the distribution of sequence lengths of all ASVs in dataset 
table(nchar(getSequences(raw_ASV_table_trimmed2)))

# What proportion is left of the sequences? 
sum(raw_ASV_table_trimmed2)/sum(raw_ASV_table2)

# Inspect the distribution of sequence lengths of all ASVs in dataset 
# AFTER TRIM
data.frame(Seq_Length = nchar(getSequences(raw_ASV_table_trimmed2))) %>%
  ggplot(aes(x = Seq_Length )) + 
  geom_histogram() + 
  labs(title = "Trimmed distribution of ASV length")
# Note the peak at 249 is ABOVE 3000

# Let's zoom in on the plot 
data.frame(Seq_Length = nchar(getSequences(raw_ASV_table_trimmed2))) %>%
  ggplot(aes(x = Seq_Length )) + 
  geom_histogram() + 
  labs(title = "Trimmed distribution of ASV length") + 
  scale_y_continuous(limits = c(0, 500))
```

I will keep these ASVs and potentially get more stringent later if it seems logical to do so.

# Remove Chimeras

Sometimes chimeras arise in our workflow. 

**Chimeric sequences** are artificial sequences formed by the combination of two or more distinct biological sequences. These chimeric sequences can arise during the polymerase chain reaction (PCR) amplification step of the 16S rRNA gene, where fragments from different templates can be erroneously joined together.

Chimera removal is an essential step in the analysis of 16S sequencing data to improve the accuracy of downstream analyses, such as taxonomic assignment and diversity assessment. It helps to avoid the inclusion of misleading or spurious sequences that could lead to incorrect biological interpretations.

```{r rm_chimeras-2022, fig.width=3.5, fig.height=3}
# Remove the chimeras in the raw ASV table
noChimeras_ASV_table2 <- removeBimeraDenovo(raw_ASV_table_trimmed2, 
                                           method="consensus", 
                                           multithread=TRUE, verbose=TRUE)

# Check the dimensions
dim(noChimeras_ASV_table2)

# What proportion is left of the sequences? 
sum(noChimeras_ASV_table2)/sum(raw_ASV_table_trimmed2)
sum(noChimeras_ASV_table2)/sum(raw_ASV_table2)

# Plot it 
data.frame(Seq_Length_NoChim2 = nchar(getSequences(noChimeras_ASV_table2))) %>%
  ggplot(aes(x = Seq_Length_NoChim2 )) + 
  geom_histogram()+ 
  labs(title = "Trimmed + Chimera Removal distribution of ASV length")

```

We retained 70% after trimming chimeras

# Track the read counts
Here, we will look at the number of reads that were lost in the filtering, denoising, merging, and chimera removal. 
```{r track_reads-2022, fig.width=6, fig.height=4}
# A little function to identify number seqs 
getN2 <- function(x) sum(getUniques(x))

# Make the table to track the seqs 
track2 <- cbind(filtered_reads2, 
               sapply(dada_forward2, getN2),
               sapply(dada_reverse2, getN2),
               sapply(merged_ASVs2, getN2),
               rowSums(noChimeras_ASV_table2))

head(track2)

# Update column names to be more informative (most are missing at the moment!)
colnames(track2) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nochim")
rownames(track2) <- samples2

# Generate a dataframe to track the reads through our DADA2 pipeline
track_counts_df2 <- 
  track2 %>%
  # make it a dataframe
  as.data.frame() %>%
  rownames_to_column(var = "names") %>%
  mutate(perc_reads_retained = 100 * nochim / input)

# Visualize it in table format 
DT::datatable(track_counts_df2)

# Plot it!
track_counts_df2 %>%
  pivot_longer(input:nochim, names_to = "read_type", values_to = "num_reads") %>%
  mutate(read_type = fct_relevel(read_type, 
                                 "input", "filtered", "denoisedF", "denoisedR", "merged", "nochim")) %>%
  ggplot(aes(x = read_type, y = num_reads, fill = read_type)) + 
  geom_line(aes(group = names), color = "grey") + 
  geom_point(shape = 21, size = 3, alpha = 0.8) + 
  scale_fill_brewer(palette = "Spectral") + 
  labs(x = "Filtering Step", y = "Number of Sequences") + 
  theme_bw()
```

There is quite a drastic drop from raw to filtered, trimmed, no chimeras data, but it seems like the sequencing depth was enough that this will still be acceptable.

# For 2023 Data

# Load Data
```{r load-data-2023}
# Set the raw fastq path to the raw sequencing files 
# Path to the fastq files 
raw_fastqs_path3 <- "01_DADA_2/data/rawfastqs/data_2023"
raw_fastqs_path3
# What files are in this path? Intuition Check 
head(list.files(raw_fastqs_path3))
# How many files are there? 
str(list.files(raw_fastqs_path3))
```

# Create vector of forward reads
```{r forward-reads-2023}
forward_reads3 <- list.files(raw_fastqs_path3, pattern = "R1.fastq.gz", full.names = TRUE)  
# Intuition Check 
head(forward_reads3)  
```

# Create a vector of reverse reads
```{r reverse-reads-2023}
reverse_reads3 <- list.files(raw_fastqs_path3, pattern = "R2.fastq.gz", full.names = TRUE)
head(reverse_reads3)
```

# Assess Raw Read Quality

##Evaluate raw sequence quality prior to trimming
### Plot 12 random samples of plots 
```{r raw-quality-plot-2023, fig.width=12, fig.height=8}
# Randomly select 12 samples from dataset to evaluate 
random_samples3 <- sample(1:length(reverse_reads3), size = 12)
random_samples3

# Calculate and plot quality of these twelve samples
forward_plot_12_3 <- plotQualityProfile(forward_reads3[random_samples3]) + 
  labs(title = "Forward Read: Raw Quality 2023")

reverse_plot_12_3 <- plotQualityProfile(reverse_reads3[random_samples3]) + 
  labs(title = "Reverse Read: Raw Quality 2023")


# Plot them together with patchwork
forward_plot_12_3 + reverse_plot_12_3
```

### Aggregated Raw Quality Plots 
```{r raw-aggregate-plot-2023, fig.width=5.5, fig.height=3.5}
# Aggregate all QC plots 
# Forward reads
forward_preQC_plot3 <- 
  plotQualityProfile(forward_reads3, aggregate = TRUE) + 
  labs(title = "Forward Pre-QC 2023")

# reverse reads
reverse_preQC_plot3 <- 
  plotQualityProfile(reverse_reads3, aggregate = TRUE) + 
  labs(title = "Reverse Pre-QC 2023")

preQC_aggregate_plot3 <- 
  # Plot the forward and reverse together 
  forward_preQC_plot3 + reverse_preQC_plot3

# Show the plot
preQC_aggregate_plot3
```

# Quality looks pretty great.  A few very low at the beginning of the forward reads.

## Prepare a placeholder for filtered reads 
```{r prep-filtered-sequences-2023}
# vector of our samples, extract sample name from files 
samples3 <- paste0(sapply(strsplit(basename(forward_reads3), "_"), `[`,3), "_2023")
# Intuition Check 
head(samples3)

# Place filtered reads into filtered_fastqs_path
filtered_fastqs_path3 <- "01_DADA_2/data/02_filtered_fastqs"
filtered_fastqs_path3

# create 2 variables: filtered_F, filtered_R
filtered_forward_reads3 <- 
  file.path(filtered_fastqs_path3, paste0(samples3, "_R1_filtered_2023.fastq.gz"))
length(filtered_forward_reads3)

# reverse reads
filtered_reverse_reads3 <- 
  file.path(filtered_fastqs_path3, paste0(samples3, "_R2_filtered_2023.fastq.gz"))
head(filtered_reverse_reads3)
```

# Filter and Trim Reads
```{r Filter-trim-2023}
filtered_reads3<-filterAndTrim(forward_reads3, filtered_forward_reads3,
              reverse_reads3, filtered_reverse_reads3,   
              truncLen = c(247,247), trimLeft = c(17,22),
              maxN = 0, maxEE = c(2,2), truncQ = 2, 
              rm.phix = TRUE, compress = TRUE, 
              multithread = TRUE)
```

# Assess Trimmed Read Quality
```{r filterTrim-quality-plots-2023,  fig.width=12, fig.height=8}
# Plot the 12 random samples after QC
forward_filteredQual_plot_12_3 <- 
  plotQualityProfile(filtered_forward_reads3[random_samples3]) + 
  labs(title = "Trimmed Forward Read Quality 2023")

reverse_filteredQual_plot_12_3 <- 
  plotQualityProfile(filtered_reverse_reads3[random_samples3]) + 
  labs(title = "Trimmed Reverse Read Quality 2023")

# Put the two plots together 
forward_filteredQual_plot_12_3 + reverse_filteredQual_plot_12_3
```

## Aggregated Trimmed Plots 
```{r qc-aggregate-plot-2023, fig.width=5.5, fig.height=3.5}
# Aggregate all QC plots 
# Forward reads
forward_postQC_plot3 <- 
  plotQualityProfile(filtered_forward_reads3, aggregate = TRUE) + 
  labs(title = "Forward Post-QC")

# reverse reads
reverse_postQC_plot3 <- 
  plotQualityProfile(filtered_reverse_reads3, aggregate = TRUE) + 
  labs(title = "Reverse Post-QC")

postQC_aggregate_plot3 <- 
  # Plot the forward and reverse together 
  forward_postQC_plot3 + reverse_postQC_plot3

# Show the plot
postQC_aggregate_plot3
```

Better quality, there are no areas of concern-- all above ~35.

## Stats on read output from `filterAndTrim`

```{r filterTrim-stats-2023}
# Make output into dataframe 
filtered_df3 <- as.data.frame(filtered_reads3)
head(filtered_df3)


# calculate some stats 
filtered_df3 %>%
  reframe(median_reads_in3 = median(reads.in),
          median_reads_out3 = median(reads.out),
          median_percent_retained3 = (median(reads.out)/median(reads.in)))
```

This time we retained 84% of reads, which is much better percentage-wise, but our average sequencing depth is MUCH lower-- 2022 numbers were 100,000s 2023 were 3000s.

### Visualize QC differences in plot 
```{r pre-post-QC-plot-2023, fig.width=6, fig.height=5.5}
# Plot the pre and post together in one plot
preQC_aggregate_plot3 / postQC_aggregate_plot3
```

The  quality improved after trimming and filtering.

# Error Modelling 

<<<<<<< HEAD
**Note every sequencing run needs to be run separately!** The error model was run separately on each Illumina dataset. This is because every Illumina run is different, even if the flow cell and DNA/samples are the same. I am doing the exact same `filterAndTrim()` step *AND*, very importantly, you'll have the same primer/ASV length/16S location expected by the output. 
=======
**Note every sequencing run needs to be run separately!** The error model *MUST* be run separately on each Illumina dataset. This is because every Illumina run is different, even if the flow cell and DNA/samples are the same. If you'd like to combine the datasets from multiple Illumina sequencing runs, you'll need to do the exact same `filterAndTrim()` step *AND*, very importantly, you'll need to have the same primer/ASV length/16S location expected by the output. 

*But wait: what contributes to sequencing error in different sequencing runs and why do we need to model errors separately per run with learnErrors() in dada2?* Remember the core principles of how Illumina seuqencing works! Some things that contribute to this are:  

-Different timings for when clusters go out of sync (drop in quality at end of reads that's typical of Illumina sequencing)
- The cluster density is impossible to exactly replicate. Therefore, the cluster density (and therefore sequence quality) will always be different between sequencing runs (even if it's the same person/samples/sequencing facility!). 
-PhiX spike-in will also vary between runs, even if we try to make it the same! Therefore, the amount of heterogeneity on the flow cell will also be different, impacting the quality.  
-Different locations on the flow cell can be impacted differently between runs. Perhaps an air bubble can get in, the cluster density happened to be higher/lower on a different run/flow cell.

Ok, that said. Let's now infer error rates for all possible transitions within purines and pyrimidines (A<>G or C<>T) and transversions between all purine and pyrimidine combinations. The error model is learned by alternating estimation of the error rates and inference of sample composition until they converge. This specifically:  

1. Starts with the assumption that the error rates are the maximum (takes the most abundant sequence ("center") and assumes it's the only sequence not caused by errors).  
2. Compares the other sequences to the most abundant sequence. 
3. Uses at most 10^8^ nucleotides for the error estimation.  
4. Uses parametric error estimation function of loess fit on the observed error rates. 
>>>>>>> 0debb33ca2b2f00a0a0c3f1f72ec87fd1329555c

## Learn the errors 
```{r learn-errors-2023, fig.width=12, fig.height=8}
# Forward reads 
error_forward_reads3 <- 
  learnErrors(filtered_forward_reads3, multithread = TRUE)
# Plot Forward  
forward_error_plot3 <- 
  plotErrors(error_forward_reads3, nominalQ = TRUE) + 
  labs(title = "Forward Read Error Model")

# Reverse reads 
error_reverse_reads3 <- 
  learnErrors(filtered_reverse_reads3, multithread = TRUE)
# Plot reverse
reverse_error_plot3 <- 
  plotErrors(error_reverse_reads3, nominalQ = TRUE) + 
  labs(title = "Reverse Read Error Model")

# Put the two plots together
forward_error_plot3 + reverse_error_plot3
```

The dots more or less align with the black lines, though there are a couple of outliers.  
<<<<<<< HEAD
The estimated error rates (black line) are a "reasonably good" fit to the observed rates (points), and the error rates drop with increased quality as expected.  We can now infer ASVs! 
=======

Details of the plot: 
- **Points**: The observed error rates for each consensus quality score.  
- **Black line**: Estimated error rates after convergence of the machine-learning algorithm.  
- **Red line:** The error rates expected under the nominal definition of the Q-score.  

Similar to what is mentioned in the dada2 tutorial: the estimated error rates (black line) are a "reasonably good" fit to the observed rates (points), and the error rates drop with increased quality as expected.  We can now infer ASVs! 
>>>>>>> 0debb33ca2b2f00a0a0c3f1f72ec87fd1329555c

# Infer ASVs 

**An important note:** This process occurs separately on forward and reverse reads! This is quite a different approach from how OTUs are identified in Mothur and also from UCHIME, oligotyping, and other OTU, MED, and ASV approaches.

```{r infer-ASVs-2023}
# Infer ASVs on the forward sequences
dada_forward3 <- dada(filtered_forward_reads3,
                     err = error_forward_reads3, 
                     multithread = TRUE)

typeof(dada_forward3)
# Grab a sample and look at it 
dada_forward3$`N2B_2023_R1_filtered_2023.fastq.gz`

# Infer ASVs on the reverse sequences 
dada_reverse3 <- dada(filtered_reverse_reads3,
                     err = error_reverse_reads3,
                     multithread = TRUE)

# Inspect 
dada_reverse3[1]
dada_reverse3[30]
```

# Merge Forward & Reverse ASVs

Now, merge the forward and reverse ASVs into contigs. 

```{r merge-ASVs-2023}
# merge forward and reverse ASVs
merged_ASVs3 <- mergePairs(dada_forward3, filtered_forward_reads3, 
                          dada_reverse3, filtered_reverse_reads3,
                          verbose = TRUE)

# Evaluate the output 
typeof(merged_ASVs3)
length(merged_ASVs3)
names(merged_ASVs3)

# Inspect the merger data.frame from the 20210602-MA-ABB1P 
head(merged_ASVs3[[3]])
```


# Create Raw ASV Count Table 
```{r generate-ASV-table-2023, fig.width=3.5, fig.height=3}
# Create the ASV Count Table 
raw_ASV_table3 <- makeSequenceTable(merged_ASVs3)

# Write out the file to data/01_DADA2


# Check the type and dimensions of the data
dim(raw_ASV_table3)
class(raw_ASV_table3)
typeof(raw_ASV_table3)

# Inspect the distribution of sequence lengths of all ASVs in dataset 
table(nchar(getSequences(raw_ASV_table3)))

# Inspect the distribution of sequence lengths of all ASVs in dataset 
# AFTER TRIM
data.frame(Seq_Length = nchar(getSequences(raw_ASV_table3))) %>%
  ggplot(aes(x = Seq_Length )) + 
  geom_histogram() + 
  labs(title = "Raw distribution of ASV length 2023")
```

###################################################
###################################################
# TRIM THE ASVS
<<<<<<< HEAD
# Let's trim the ASVs to only be the right size, which around 400.
=======
# Let's trim the ASVs to only be the right size, which is 249.
# 249 originates from our expected amplicon of 252 - 3bp in the forward read due to low quality.
>>>>>>> 0debb33ca2b2f00a0a0c3f1f72ec87fd1329555c
``` {r trimming-ASVs-2023}
# We will allow for a few 
raw_ASV_table_trimmed3 <- raw_ASV_table3[,nchar(colnames(raw_ASV_table3)) %in% 399:436]

# Inspect the distribution of sequence lengths of all ASVs in dataset 
table(nchar(getSequences(raw_ASV_table_trimmed3)))

# What proportion is left of the sequences? 
sum(raw_ASV_table_trimmed3)/sum(raw_ASV_table3)

# Inspect the distribution of sequence lengths of all ASVs in dataset 
# AFTER TRIM
data.frame(Seq_Length = nchar(getSequences(raw_ASV_table_trimmed3))) %>%
  ggplot(aes(x = Seq_Length )) + 
  geom_histogram() + 
  labs(title = "Trimmed distribution of ASV length")
# Note the peak at 249 is ABOVE 3000

# Let's zoom in on the plot 
data.frame(Seq_Length = nchar(getSequences(raw_ASV_table_trimmed3))) %>%
  ggplot(aes(x = Seq_Length )) + 
  geom_histogram() + 
  labs(title = "Trimmed distribution of ASV length") + 
  scale_y_continuous(limits = c(0, 500))
```

For now I will leave a generous number of ASVs, but in the future I could restrict the ASV lengths allowed further.


# Remove Chimeras

Sometimes chimeras arise in our workflow. 

**Chimeric sequences** are artificial sequences formed by the combination of two or more distinct biological sequences. These chimeric sequences can arise during the polymerase chain reaction (PCR) amplification step of the 16S rRNA gene, where fragments from different templates can be erroneously joined together.

Chimera removal is an essential step in the analysis of 16S sequencing data to improve the accuracy of downstream analyses, such as taxonomic assignment and diversity assessment. It helps to avoid the inclusion of misleading or spurious sequences that could lead to incorrect biological interpretations.

```{r rm_chimeras-2023, fig.width=3.5, fig.height=3}
# Remove the chimeras in the raw ASV table
noChimeras_ASV_table3 <- removeBimeraDenovo(raw_ASV_table_trimmed3, 
                                           method="consensus", 
                                           multithread=TRUE, verbose=TRUE)

# Check the dimensions
dim(noChimeras_ASV_table3)

# What proportion is left of the sequences? 
sum(noChimeras_ASV_table3)/sum(raw_ASV_table_trimmed3)
sum(noChimeras_ASV_table3)/sum(raw_ASV_table3)

# Plot it 
data.frame(Seq_Length_NoChim = nchar(getSequences(noChimeras_ASV_table3))) %>%
  ggplot(aes(x = Seq_Length_NoChim )) + 
  geom_histogram()+ 
  labs(title = "Trimmed + Chimera Removal distribution of ASV length")
# Note the difference in the peak at 249, which is now BELOW 3000
```

95% of both our forward and reverse reads were conserved.

# Track the read counts
Here, we will look at the number of reads that were lost in the filtering, denoising, merging, and chimera removal. 
```{r track_reads-2023, fig.width=6, fig.height=4}
# A little function to identify number seqs 
getN3 <- function(x) sum(getUniques(x))

# Make the table to track the seqs 
track3 <- cbind(filtered_reads3, 
               sapply(dada_forward3, getN3),
               sapply(dada_reverse3, getN3),
               sapply(merged_ASVs3, getN3),
               rowSums(noChimeras_ASV_table3))

head(track3)

# Update column names to be more informative (most are missing at the moment!)
colnames(track3) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nochim")
rownames(track3) <- samples3
track3

# Generate a dataframe to track the reads through our DADA2 pipeline
track_counts_df3 <- 
  track3 %>%
  # make it a dataframe
  as.data.frame() %>%
  rownames_to_column(var = "names") %>%
  mutate(perc_reads_retained = 100 * nochim / input )

# Visualize it in table format 
DT::datatable(track_counts_df3)

# Plot it!
track_counts_df3 %>%
  pivot_longer(input:nochim, names_to = "read_type", values_to = "num_reads") %>%
  mutate(read_type = fct_relevel(read_type, 
                                 "input", "filtered", "denoisedF", "denoisedR", "merged", "nochim")) %>%
  ggplot(aes(x = read_type, y = num_reads, fill = read_type)) + 
  geom_line(aes(group = names), color = "grey") + 
  geom_point(shape = 21, size = 3, alpha = 0.8) + 
  scale_fill_brewer(palette = "Spectral") + 
  labs(x = "Filtering Step", y = "Number of Sequences") + 
  theme_bw()
```
<<<<<<< HEAD
# There is a steep line in s
=======
>>>>>>> 0debb33ca2b2f00a0a0c3f1f72ec87fd1329555c

# Merge the 2022 and 2023 ASV tables (no chimeras)

```{r merge-asv-tables}
# If multiple sequencing runs were processed:
noChimeras_ASV_table <- mergeSequenceTables(table1 = noChimeras_ASV_table2, table2 = noChimeras_ASV_table3) # Also accepts other input formats, e.g. tables provided as in-memory objects
#View(noChimeras_ASV_table)
```


# Assign Taxonomy 

Here, we will use the silva database version 138!
```{r assign-tax}
# Classify the ASVs against a reference set using the RDP Naive Bayesian Classifier described by Wang et al., (2007) in AEM
taxa_train <- 
  assignTaxonomy(noChimeras_ASV_table, 
                 "/workdir/in_class_data/taxonomy/silva_nr99_v138.1_train_set.fa.gz", 
                 multithread=TRUE)

# Add the genus/species information 
taxa_addSpecies <- 
  addSpecies(taxa_train, 
             "/workdir/in_class_data/taxonomy/silva_species_assignment_v138.1.fa.gz")

# Inspect the taxonomy 
taxa_print <- taxa_addSpecies # Removing sequence rownames for display only
rownames(taxa_print) <- NULL
#View(taxa_print)
```

# Prepare the data for export! 

## 1. ASV Table 

Below, we will prepare the following: 

1. Two ASV Count tables: 
      a. With ASV seqs: ASV headers include the *entire* ASV sequence ~250bps.
      b. with ASV names: This includes re-written and shortened headers like ASV_1, ASV_2, etc, which will match the names in our fasta file below.  
2. `ASV_fastas`: A fasta file that we can use to build a tree for phylogenetic analyses (e.g. phylogenetic alpha diversity metrics or UNIFRAC dissimilarty).  

### Finalize ASV Count Tables 
```{r prepare-ASVcount-table}
########### 2. COUNT TABLE ###############
############## Modify the ASV names and then save a fasta file!  ############## 
# Give headers more manageable names
# First pull the ASV sequences
asv_seqs <- colnames(noChimeras_ASV_table)
asv_seqs[1:5]

# make headers for our ASV seq fasta file, which will be our asv names
asv_headers <- vector(dim(noChimeras_ASV_table)[2], mode = "character")
asv_headers[1:5]

# loop through vector and fill it in with ASV names 
for (i in 1:dim(noChimeras_ASV_table)[2]) {
  asv_headers[i] <- paste(">ASV", i, sep = "_")
}

# intitution check
asv_headers[1:5]

##### Rename ASVs in table then write out our ASV fasta file! 
#View(noChimeras_ASV_table)
asv_tab <- t(noChimeras_ASV_table)
#View(asv_tab)

## Rename our asvs! 
row.names(asv_tab) <- sub(">", "", asv_headers)
#View(asv_tab)
```

## 2. Taxonomy Table 
```{r prepare-tax-table}
# Inspect the taxonomy table
#View(taxa_addSpecies)

##### Prepare tax table 
# Add the ASV sequences from the rownames to a column 
new_tax_tab <- 
  taxa_addSpecies%>%
  as.data.frame() %>%
  rownames_to_column(var = "ASVseqs") 
head(new_tax_tab)

# intution check 
stopifnot(new_tax_tab$ASVseqs == colnames(noChimeras_ASV_table))

# Now let's add the ASV names 
rownames(new_tax_tab) <- rownames(asv_tab)
head(new_tax_tab)

### Final prep of tax table. Add new column with ASV names 
asv_tax <- 
  new_tax_tab %>%
  # add rownames from count table for phyloseq handoff
  mutate(ASV = rownames(asv_tab)) %>%
  # Resort the columns with select
  dplyr::select(Kingdom, Phylum, Class, Order, Family, Genus, Species, ASV, ASVseqs)

head(asv_tax)

# Intution check
stopifnot(asv_tax$ASV == rownames(asv_tax), rownames(asv_tax) == rownames(asv_tab))
```


# Write `01_DADA2` files

Now, we will write the files! We will write the following to the `data/01_DADA2/` folder. We will save both as files that could be submitted as supplements AND as .RData objects for easy loading into the next steps into R.:  

1. `ASV_counts.tsv`: ASV count table that has ASV names that are re-written and shortened headers like ASV_1, ASV_2, etc, which will match the names in our fasta file below. This will also be saved as `data/01_DADA2/ASV_counts.RData`.
2. `ASV_counts_withSeqNames.tsv`: This is generated with the data object in this file known as `noChimeras_ASV_table`. ASV headers include the *entire* ASV sequence ~250bps.  In addition, we will save this as a .RData object as `data/01_DADA2/noChimeras_ASV_table.RData` as we will use this data in `analysis/02_Taxonomic_Assignment.Rmd` to assign the taxonomy from the sequence headers.  
3. `ASVs.fasta`: A fasta file output of the ASV names from `ASV_counts.tsv` and the sequences from the ASVs in `ASV_counts_withSeqNames.tsv`. A fasta file that we can use to build a tree for phylogenetic analyses (e.g. phylogenetic alpha diversity metrics or UNIFRAC dissimilarty).  
4. We will also make a copy of `ASVs.fasta` in `data/02_TaxAss_FreshTrain/` to be used for the taxonomy classification in the next step in the workflow.  
5. Write out the taxonomy table
6. `track_read_counts.RData`: To track how many reads we lost throughout our workflow that could be used and plotted later. We will add this to the metadata in `analysis/02_Taxonomic_Assignment.Rmd`.   


```{r save-files}
# FIRST, we will save our output as regular files, which will be useful later on. 
# Save to regular .tsv file 
# Write BOTH the modified and unmodified ASV tables to a file!
# Write count table with ASV numbered names (e.g. ASV_1, ASV_2, etc)
write.table(asv_tab, "data/01_DADA2/ASV_counts.tsv", sep = "\t", quote = FALSE, col.names = NA)
# Write count table with ASV sequence names
write.table(noChimeras_ASV_table, "data/01_DADA2/ASV_counts_withSeqNames.tsv", sep = "\t", quote = FALSE, col.names = NA)
# Write out the fasta file for reference later on for what seq matches what ASV
asv_fasta <- c(rbind(asv_headers, asv_seqs))
# Save to a file!
write(asv_fasta, "data/01_DADA2/ASVs.fasta")


# SECOND, let's save the taxonomy tables 
# Write the table 
write.table(asv_tax, "data/01_DADA2/ASV_taxonomy.tsv", sep = "\t", quote = FALSE, col.names = NA)


# THIRD, let's save to a RData object 
# Each of these files will be used in the analysis/02_Taxonomic_Assignment
# RData objects are for easy loading :) 
save(noChimeras_ASV_table, file = "data/01_DADA2/noChimeras_ASV_table.RData")
save(asv_tab, file = "data/01_DADA2/ASV_counts.RData")
# And save the track_counts_df a R object, which we will merge with metadata information in the next step of the analysis in nalysis/02_Taxonomic_Assignment. 
save(track_read_counts, file = "data/01_DADA2/track_read_counts.RData")
```

# Session Information 
```{r session-info}
# Ensure reproducibility 
devtools::session_info()
```